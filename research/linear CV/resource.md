## Linear attention
[crash course](https://zhuanlan.zhihu.com/p/718156896)
[线性Attention的探索：Attention必须有个Softmax吗？ - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/7546)

Canonical attantion:  $O(L^2 d)$ , linear attention: $O(d^2L)$
![[Pasted image 20241123115846.png]]
GAU
Retnet
GLA
## mamba
[一文通透mamba2「力证Transformer are SSM」：从SSM、半可分矩阵、SMA、SSD到mamba2-CSDN博客](https://blog.csdn.net/v_JULY_v/article/details/140131413)
## Triton


