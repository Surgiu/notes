## Linear attention
[crash course](https://zhuanlan.zhihu.com/p/718156896)
[线性Attention的探索：Attention必须有个Softmax吗？ - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/7546)

![[Pasted image 20241123104023.png]]
Canonical attantion:  $O(L^2 d)$ , linear attention: $O(d^2L)$
GAU
Retnet
GLA
## mamba

## Triton


